{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#os.environ[\"TIREX_NO_CUDA\"] = \"1\" # Turns off slstm cuda kernels if you have problems but be aware of the downsides! (see Repository FAQ)\n",
    "from tirex import load_model, ForecastModel\n",
    "from util_plot import plot_fc\n",
    "\n",
    "# Load Example Data\n",
    "ctx_s, future_s = np.split(np.genfromtxt(Path.cwd() / \"air_passengers.csv\"), [-12])\n",
    "ctx_l, future_l = np.split(np.genfromtxt(Path.cwd() / \"loop_seattle_5T_example.csv\"), [-512])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TiRex Forecast in 2 Lines\n",
    "\n",
    "1) Load Model\n",
    "2) Generate Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load Model\n",
    "model: ForecastModel = load_model(\"NX-AI/TiRex\")\n",
    "\n",
    "# 2) Short Horizion - Example\n",
    "quantiles, mean = model.forecast(ctx_s, prediction_length=24)\n",
    "plot_fc(ctx_s, quantiles[0], future_s)\n",
    "\n",
    "# 2) Long Horizion - Example\n",
    "quantiles, mean = model.forecast(ctx_l, prediction_length=768)\n",
    "plot_fc(ctx_l, quantiles[0], future_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Options\n",
    "\n",
    "TiRex supports forecasting with different input types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(np.genfromtxt(Path.cwd() / \"air_passengers.csv\")) # Load Example\n",
    "\n",
    "# Torch tensor (2D or 1D)\n",
    "quantiles, means = model.forecast(context=data, prediction_length=24)\n",
    "print(\"Predictions (Torch tensor):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "# List of Torch tensors (List of 1D) - will be padded\n",
    "list_torch_data = [data, data, data] \n",
    "quantiles, means = model.forecast(context=list_torch_data, prediction_length=24, batch_size=2)\n",
    "print(\"Predictions (List of Torch tensors):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "# NumPy array (2D or 1D)\n",
    "quantiles, means = model.forecast(context=data.numpy(), prediction_length=24, output_type='torch')\n",
    "print(\"Predictions (NumPy):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# List of NumPy arrays (List of 1D) - will be padded\n",
    "list_numpy_data = [data.numpy()]  # Split into 3 sequences\n",
    "quantiles, means = model.forecast(context=list_numpy_data, prediction_length=24)\n",
    "print(\"Predictions (List of NumPy arrays):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# GluonTS Dataset \n",
    "try:\n",
    "    from typing import cast\n",
    "    from gluonts.dataset import Dataset\n",
    "    gluon_dataset = cast(Dataset, [{'target': data, \"item_id\": 1}, {'target': data, \"item_id\": 22}])\n",
    "    quantiles, means = model.forecast_gluon(gluon_dataset, prediction_length=24)\n",
    "    print(\"Predictions GluonDataset:\\n\", type(quantiles), quantiles.shape)\n",
    "    # If you use also `glutonts` as your output type the start_time and item_id get preserved accordingly\n",
    "    predictions_gluon = model.forecast_gluon(gluon_dataset, prediction_length=24, output_type=\"gluonts\")\n",
    "    print(\"Predictions GluonDataset:\\n\", type(predictions_gluon), type(predictions_gluon[0]))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # To use the gluonts function you need to install the optinal dependency\n",
    "    # pip install tirex[gluonts]\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Options\n",
    "\n",
    "\n",
    "TiRex supports different output types for the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(np.genfromtxt(Path.cwd() / \"air_passengers.csv\")) # Load Example\n",
    "\n",
    "# Default: 2D Torch tensor\n",
    "quantiles, means = model.forecast(context=data, prediction_length=24, output_type=\"torch\")\n",
    "print(\"Predictions:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "\n",
    "# 2D Numpy Array\n",
    "quantiles, means = model.forecast(context=data, prediction_length=24, output_type=\"numpy\")\n",
    "print(\"Predictions:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# Iterate by patch\n",
    "# You can also use the forecast function as iterable. This might help with big datasets. All output_types are supported\n",
    "for i, fc_batch in enumerate(model.forecast(context=[data, data, data, data, data], batch_size=2, output_type=\"torch\", yield_per_batch=True)):\n",
    "    quantiles, means = fc_batch\n",
    "    print(f\"Predictions batch {i}:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "try:\n",
    "    # QuantileForecast (GluonTS)\n",
    "    predictions_gluonts = model.forecast(context=data, prediction_length=24, output_type=\"gluonts\")\n",
    "    print(\"Predictions (GluonTS Quantile Forecast):\\n\", type(predictions_gluon), type(predictions_gluon[0]))\n",
    "    predictions_gluonts[0].plot()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # To use the gluonts function you need to install the optinal dependency\n",
    "    # pip install tirex[gluonts]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
