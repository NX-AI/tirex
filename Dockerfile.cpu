# -------------------------
# Stage 1: Build
# -------------------------
FROM python:3.11-slim AS build

# Install system dependencies (rarely changes)
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set a working directory
WORKDIR /home/model

# --- Install the TiRex Package from GitHub ---
RUN pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu && \
    git clone https://github.com/NX-AI/tirex.git && \
    cd tirex && \
    pip install -e ".[all]" && \
    pip install jupyterlab notebook

# --- Prime the Cache by Baking the Model into the Container ---
# Download the model to the default Hugging Face cache location inside the image.
# This makes it available instantly at runtime without a network call.
RUN huggingface-cli download NX-AI/TiRex model.ckpt --local-dir-use-symlinks=False


# -------------------------
# Stage 2: Runtime
# -------------------------
FROM python:3.11-slim

# --- Set working directory to match build stage ---
WORKDIR /home/model

# --- Final Server Setup ---
# Set the environment variable to disable CUDA
ENV TIREX_NO_CUDA=1

# --- Copy Python environment ---
COPY --from=build /usr/local /usr/local

# --- Copy Hugging Face model cache ---
COPY --from=build /root/.cache/huggingface /root/.cache/huggingface

COPY --from=build /home/model/tirex /home/model/tirex

# Set default command
CMD ["python", "-c", "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print('Running on CPU')"]
